{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1ee73866",
   "metadata": {},
   "source": [
    "---\n",
    "title: NaNoWriMo Word Analysis\n",
    "description: Word analysis for NaNowriMo books (built in Jupyter, shared in Mercury)\n",
    "show-code: False\n",
    "params:\n",
    "    most_freq_amount:\n",
    "        input: slider\n",
    "        label: Find X Top Words\n",
    "        value: 10\n",
    "        min: 1\n",
    "        max: 50\n",
    "    user_defined_stopwords:\n",
    "        input: text\n",
    "        label: Words to Ignore (seperate with a comma)\n",
    "        value: the, and, a, in, i\n",
    "    input_file:\n",
    "        label: Upload File (max file size = 100MB)\n",
    "        input: file\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19837824",
   "metadata": {},
   "source": [
    "## NaNoWriMo Word Analysis\n",
    "\n",
    "Most frequency word analysis and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a23fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables with default values\n",
    "most_freq_amount = 10\n",
    "user_defined_stopwords = \"the, and, a, in, i\"\n",
    "input_file = \"orwell_1984.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea3835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interactive Jupyter Notebook, running in Mercury for web application\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Frequency Distribution\n",
    "nltk.download('stopwords', quiet=True) # Stopwords for cleanup\n",
    "nltk.download('vader_lexicon', quiet=True) # Sentiment Analysis via VADER (Valence Aware Dictionary for Sentiment Reasoning)\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # Sentiment Analysis via VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the text file as tokens\n",
    "user_defined_stopwords_to_remove = user_defined_stopwords.replace(\" \", \"\").split(\",\")\n",
    "with open(input_file, \"r\") as f:\n",
    "\tfile_text = f.readlines()\n",
    "file_text = \" \".join(file_text) # combine all lines into one string\n",
    "\n",
    "# Break into Tokens and Clean-up\n",
    "string_as_tokens_lst = nltk.word_tokenize(file_text)\n",
    "print(\"Total Token Count: {0}\".format(len(string_as_tokens_lst)))\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "punctuation = [\"--\", \"'\", \"''\", \"``\", \"?\", \"!\", \".\", \",\", \";\", \")\", \"(\", \"‘\", \"●\", \":\", '“', '”', '○', \"[\", \"]\", \"&\", '’', \"%\", \"*\", \"–\", \"·\", \"-\"]\n",
    "\n",
    "# Remove and combine possesive 's\n",
    "s_loc = []\n",
    "for i, word in enumerate(string_as_tokens_lst): \n",
    "    if word == \"'s\": \n",
    "        s_loc.append(i)\n",
    "for j in s_loc: \n",
    "    string_as_tokens_lst[j-1] = \"\".join([string_as_tokens_lst[j-1], string_as_tokens_lst[j]])\n",
    "for index in sorted(s_loc, reverse=True):\n",
    "    del string_as_tokens_lst[index]\n",
    "\n",
    "# User defined stopwords\n",
    "string_as_tokens_lst = [w for w in string_as_tokens_lst if w not in stopwords] # remove stopwords\n",
    "string_as_tokens_lst = [w.lower() for w in string_as_tokens_lst if w not in punctuation] # remove punctuation\n",
    "string_as_tokens_lst = [u for u in string_as_tokens_lst if u not in user_defined_stopwords_to_remove] # removes\n",
    "print(\"Word Count (without words to be ignored): {0}\\n\".format(len(string_as_tokens_lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution\n",
    "frequency_dist = nltk.FreqDist(string_as_tokens_lst)\n",
    "frequencyDistribution_as_dict = dict(frequency_dist.most_common(most_freq_amount)) # convert to dict for plotting\n",
    "print(frequencyDistribution_as_dict)\n",
    "df = pd.DataFrame(frequencyDistribution_as_dict.items(), columns=[\"Word\", \"Occurence\"])\n",
    "df.set_index(\"Word\",drop=True,inplace=True)\n",
    "fig = px.bar(df, y=\"Occurence\", title=\"Top {0} Words\".format(most_freq_amount))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation Distribution\n",
    "def plotNGram(n_gram_amount, n_gram_finder):\n",
    "\t# Plot N-Grams\n",
    "\tnGram_as_dict_temp = dict(n_gram_finder.ngram_fd.most_common(most_freq_amount)) # convert to dict for plotting\n",
    "\n",
    "\tnGram_as_dict = {}\n",
    "\tfor k, v in nGram_as_dict_temp.items():\n",
    "\t\tnGram_as_dict[\", \".join(k)] = v # rename key from ('graduate', 'division') to \"graduate, division\"\n",
    "\tdf2 = pd.DataFrame(nGram_as_dict.items(), columns=[\"Word\", \"Occurence\"])\n",
    "\tdf2.set_index(\"Word\",drop=True,inplace=True)\n",
    "\tprint(nGram_as_dict)    \n",
    "\tfig = px.bar(df2, y=\"Occurence\", title=\"Top {0} Word {1}\".format(most_freq_amount, n_gram_amount))\n",
    "\tfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8cbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams: Two-Word Combinations\n",
    "bigram_collocation_dist = nltk.collocations.BigramCollocationFinder.from_words(string_as_tokens_lst)\n",
    "plotNGram(\"Pairs\", bigram_collocation_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams: Three-Word Combinations\n",
    "trigram_collocation_dist = nltk.collocations.TrigramCollocationFinder.from_words(string_as_tokens_lst)\n",
    "plotNGram(\"Triplets\", trigram_collocation_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e243780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadgrams: Four-Word Combinations\n",
    "quadgram_collocation_dist = nltk.collocations.QuadgramCollocationFinder.from_words(string_as_tokens_lst)\n",
    "plotNGram(\"Quadruplets\", quadgram_collocation_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d93ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "def sentimentAnalysis(plot_title_from_file_name, file_as_tokens,):\n",
    "\t# Sentiment Analysis of Pieces of X Length\n",
    "\t# VADER Citation: Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\tsize_of_sentiment_string = 15 # average length of a sentence\n",
    "\tlist_of_strings_x_length = []\n",
    "\tfor i in range(0, len(file_as_tokens), size_of_sentiment_string):\n",
    "\t\tstring_sentence = \" \".join(file_as_tokens[i:i+size_of_sentiment_string])\n",
    "\t\tlist_of_strings_x_length.append(string_sentence)\n",
    "\n",
    "\tsentiment_analyzer = SentimentIntensityAnalyzer() # Via VADER*\n",
    "\tsent_dict_postive = {}\n",
    "\tsent_dict_neutral = {}\n",
    "\tsent_dict_negative = {}\n",
    "\tfor i, string_sent in enumerate(list_of_strings_x_length):\n",
    "\t\tsent_dict_for_sentence = sentiment_analyzer.polarity_scores(string_sent)\n",
    "\t\tsent_dict_postive[i] = sent_dict_for_sentence[\"pos\"]\n",
    "\t\tsent_dict_neutral[i] = sent_dict_for_sentence[\"neu\"]\n",
    "\t\tsent_dict_negative[i] = sent_dict_for_sentence[\"neg\"]\n",
    "\n",
    "\tcolor_plot = {\"Postive\": \"Reds\", \"Negative\": \"Blues\", \"Neutral\": \"gray\"} # colors for plot (cmap)\n",
    "\n",
    "\t# Plot Sentiment Individually\n",
    "\tdef plotSentimentIndvidually(polarity_name, polarity_dict):\n",
    "\t\t# Plot\n",
    "\t\tfig = plt.figure(figsize=(12,12), dpi=100)\n",
    "\t\tplt.title(\"{0}: {1} Sentiment\".format(plot_title_from_file_name, polarity_name))\n",
    "\t\tplt.scatter(polarity_dict.keys(), polarity_dict.values(), c=[i * 10 for i in polarity_dict.values()], cmap=color_plot[polarity_name])\n",
    "\t\tplt.xticks(rotation=90)\n",
    "\t\tplt.xlabel(\"Sentence Piece\")\n",
    "\t\tplt.ylabel(\"{0} Sentiment %\".format(polarity_name))\n",
    "\tplotSentimentIndvidually(\"Postive\", sent_dict_postive)\n",
    "\t#plotSentimentIndvidually(\"Neutral\", sent_dict_neutral)\n",
    "\tplotSentimentIndvidually(\"Negative\", sent_dict_negative)\n",
    "\n",
    "\t# Plot as Group\n",
    "\tfig = plt.figure(figsize=(12,12), dpi=100)\n",
    "\tplt.title(\"{0}: Postive and Negative Sentiment\".format(plot_title_from_file_name))\n",
    "\tplt.scatter(sent_dict_postive.keys(), sent_dict_postive.values(), c=[i * 10 for i in sent_dict_postive.values()], cmap=color_plot[\"Postive\"])\n",
    "\t#plt.scatter(sent_dict_neutral.keys(), sent_dict_neutral.values(), c=[i * 10 for i in sent_dict_neutral.values()], cmap=color_plot[\"Neutral\"])\n",
    "\tplt.scatter(sent_dict_negative.keys(), sent_dict_negative.values(), c=[i * 10 for i in sent_dict_negative.values()], cmap=color_plot[\"Negative\"])\n",
    "\tplt.xticks(rotation=90)\n",
    "\tplt.xlabel(\"Sentence #\")\n",
    "\tplt.ylabel(\"Sentiment %\")\n",
    "\n",
    "\t# Plot Trends by Ploting a Line Graph for Every X\n",
    "\tsize_of_text_chunks = len(sent_dict_postive.keys()) - 1\n",
    "\taverage_every_x = 25 # average the sentiment of x sentences\n",
    "\taverage_pos_dict = {}\n",
    "\taverage_neg_dict = {}\n",
    "\tfor i in range(0, size_of_text_chunks, math.floor(size_of_text_chunks/average_every_x)):\n",
    "\t\tpos_polarity_values = []\n",
    "\t\tneg_polarity_values = []\n",
    "\t\tfor j in range(i, i+average_every_x-1):\n",
    "\t\t\tif j in sent_dict_postive.keys() and j in sent_dict_negative.keys():\n",
    "\t\t\t\tpos_polarity_values.append(sent_dict_postive[j])\n",
    "\t\t\t\tneg_polarity_values.append(sent_dict_negative[j])\n",
    "\t\taverage_pos_dict[i] = sum(pos_polarity_values) / len(pos_polarity_values)\n",
    "\t\taverage_neg_dict[i] = sum(neg_polarity_values) / len(neg_polarity_values)\n",
    "\n",
    "\tfig = plt.figure(figsize=(12,12), dpi=100)\n",
    "\tplt.title(\"{0}: Trends in Sentiment\".format(plot_title_from_file_name))\n",
    "\tplt.scatter(sent_dict_postive.keys(), sent_dict_postive.values(), c=[i * 10 for i in sent_dict_postive.values()], cmap=color_plot[\"Postive\"])\n",
    "\tplt.scatter(sent_dict_negative.keys(), sent_dict_negative.values(), c=[i * 10 for i in sent_dict_negative.values()], cmap=color_plot[\"Negative\"])\n",
    "\tplt.plot(list(average_pos_dict.keys()), list(average_pos_dict.values()), c=\"red\")\n",
    "\tplt.plot(list(average_neg_dict.keys()), list(average_neg_dict.values()), c=\"blue\")\n",
    "\tplt.xticks(rotation=90)\n",
    "\tplt.xlabel(\"Sentence #\")\n",
    "\tplt.ylabel(\"Sentiment %\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16412630",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_title = input_file.split(\".\")[0]\n",
    "sentimentAnalysis(book_title, string_as_tokens_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234454c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea692fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

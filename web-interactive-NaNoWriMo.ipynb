{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1ee73866",
   "metadata": {},
   "source": [
    "---\n",
    "title: NaNoWriMo Word Analysis\n",
    "description: Word analysis for NaNowriMo books (built in Jupyter, shared in Mercury)\n",
    "show-code: False\n",
    "params:\n",
    "    most_freq_amount:\n",
    "        input: slider\n",
    "        label: Find X Top Words\n",
    "        value: 10\n",
    "        min: 1\n",
    "        max: 50\n",
    "    user_defined_stopwords:\n",
    "        input: text\n",
    "        label: Words to Ignore (seperate with a comma)\n",
    "        value: the, and, a, in, i\n",
    "    input_file:\n",
    "        label: Upload File (max file size = 100MB)\n",
    "        input: file\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea3835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interactive Jupyter Notebook, running in Mercury for web application\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt', quiet=True) # Frequency Distribution\n",
    "#nltk.download('stopwords', quiet=True) # Stopwords for cleanup\n",
    "#nltk.download('vader_lexicon', quiet=True) # Sentiment Analysis via VADER (Valence Aware Dictionary for Sentiment Reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the text file as tokens\n",
    "user_defined_stopwords_to_remove = user_defined_stopwords.replace(\" \", \"\").split(\",\")\n",
    "with open(input_file, \"r\") as f:\n",
    "\tfile_text = f.readlines()\n",
    "file_text = \" \".join(file_text) # combine all lines into one string\n",
    "print(file_text)\n",
    "\n",
    "# Break into Tokens and Clean-up\n",
    "string_as_tokens_lst = nltk.word_tokenize(file_text)\n",
    "print(\"Total Token Count: {0}\".format(len(string_as_tokens_lst)))\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "punctuation = [\"--\", \"'\", \"''\", \"``\", \"?\", \"!\", \".\", \",\", \";\", \")\", \"(\", \"‘\", \"●\", \":\", '“', '”', '○', \"[\", \"]\", \"&\", '’', \"%\", \"*\", \"–\", \"·\", \"-\"]\n",
    "\n",
    "# User defined stopwords\n",
    "string_as_tokens_lst = [w for w in string_as_tokens_lst if w not in stopwords] # remove stopwords\n",
    "string_as_tokens_lst = [w.lower() for w in string_as_tokens_lst if w not in punctuation] # remove punctuation\n",
    "string_as_tokens_lst = [u for u in string_as_tokens_lst if u not in user_defined_stopwords_to_remove] # removes\n",
    "print(\"Word Count (without words to be ignored): {0}\\n\".format(len(string_as_tokens_lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution\n",
    "frequency_dist = nltk.FreqDist(string_as_tokens_lst)\n",
    "frequencyDistribution_as_dict = dict(frequency_dist.most_common(most_freq_amount)) # convert to dict for plotting\n",
    "print(frequencyDistribution_as_dict)\n",
    "df = pd.DataFrame(frequencyDistribution_as_dict.items(), columns=[\"Word\", \"Occurence\"])\n",
    "df.set_index(\"Word\",drop=True,inplace=True)\n",
    "fig = px.bar(df, y=\"Occurence\", title=\"Top Words\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation Distribution\n",
    "def plotNGram(n_gram_amount, n_gram_finder):\n",
    "\t# Plot N-Grams\n",
    "\tnGram_as_dict_temp = dict(n_gram_finder.ngram_fd.most_common(most_freq_amount)) # convert to dict for plotting\n",
    "\n",
    "\tnGram_as_dict = {}\n",
    "\tfor k, v in nGram_as_dict_temp.items():\n",
    "\t\tnGram_as_dict[\", \".join(k)] = v # rename key from ('graduate', 'division') to \"graduate, division\"\n",
    "\tdf2 = pd.DataFrame(nGram_as_dict.items(), columns=[\"Word\", \"Occurence\"])\n",
    "\tdf2.set_index(\"Word\",drop=True,inplace=True)\n",
    "\tfig = px.bar(df2, y=\"Occurence\", title=\"Top Word {0}\".format(n_gram_amount))\n",
    "\tfig.show()\n",
    "\tprint(nGram_as_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8cbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams: Two-Word Combinations\n",
    "bigram_collocation_dist = nltk.collocations.BigramCollocationFinder.from_words(string_as_tokens_lst)\n",
    "plotNGram(\"Pairs\", bigram_collocation_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams: Three-Word Combinations\n",
    "trigram_collocation_dist = nltk.collocations.TrigramCollocationFinder.from_words(string_as_tokens_lst)\n",
    "plotNGram(\"Triplets\", trigram_collocation_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e243780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadgrams: Four-Word Combinations\n",
    "quadgram_collocation_dist = nltk.collocations.QuadgramCollocationFinder.from_words(string_as_tokens_lst)\n",
    "plotNGram(\"Quadruplets\", quadgram_collocation_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d93ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16412630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
